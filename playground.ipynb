{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- data_filter: \n",
    "  - in loss: masked positions are zeroed for computed prediction before loss is computed `_define_loss(self)`\n",
    "    - -> masked outputs should be 0 for loss to work correctly \n",
    "    - -> loss becomes 0 because 0\\*log(0)-0, similar for gaussian, ...\n",
    "  - ignored by `get_null_ll`, ignored by `poisson_unit_norm` normalization\n",
    "\n",
    "- cost `_define_loss`:\n",
    "  - `unit_cost` is summed across batch, divided by `cost_norm` or `nt` (batch_size) -> LL/sample\n",
    "  - `cost_norm` is only for poisson, `nt` or `nt`\\*`poisson_unit_norm` (avg. spike for neuron) -> makes cost LL/spike\n",
    "    - -> normalizes per expected with expected number of spikes per batch for indiv. neuron (avg \\* batch_size)\n",
    "  - `cost` is the same summed across output dim as well\n",
    "  \n",
    "- eval_models `eval_models`:\n",
    "  - `unit_cost` summed across batches and divided by number of (all) batches `...np.divide(unit_cost, num_batches_test)`\n",
    "  - `get_null_ll` doesn't take `data_filter` into account\n",
    "  \n",
    "- Layers:\n",
    "  - Layers are connected via \"flattened\" interface, \n",
    "    - conv2 layers always have reshape to flat -> 2D in the beginning, 2D -> flat at the end\n",
    "    - actual dimensions for inter-layers reshapes stored in `layer_size`\n",
    "\n",
    "- ConvLayer: \n",
    "  - computes intput size/shape based on `layer_sizes[nn]` (`ffnetwork.py::_define_network`), 0 is `input_dims`\n",
    "  - number of filters is based on `layer_size[nn+1]`\n",
    "  - updates `layer_size[nn+1]` (next's input size) to current `layers[nn].output_dims`\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is in float64 format: 0.0-0.000255\n",
    "def load_data(type, region):\n",
    "    # `raw_validation_set.npy` is multiple tries non-averaged `validation_set.npy`\n",
    "    return (\n",
    "        (np.load(f'./Data/region{region}/{type}_inputs.npy')*1_000_000).astype(np.uint8),\n",
    "        np.load(f'./Data/region{region}/{type}_set.npy')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_and_get_mask(inputs, outputs):\n",
    "    # Assumes output is flat vector\n",
    "    # Assumes result neurons for individual outputs are independent i.e. first result neuron of first output\n",
    "    # ..is different from first neuron of second output\n",
    "    assert len(inputs) == len(outputs)\n",
    "    \n",
    "    total_len = 0\n",
    "    total_output_dim = 0\n",
    "    for i in range(len(inputs)):\n",
    "        assert inputs[i].shape[0] == outputs[i].shape[0]\n",
    "        total_len += inputs[i].shape[0]\n",
    "        total_output_dim += outputs[i].shape[1]\n",
    "        \n",
    "    merged_inputs = np.concatenate(inputs, axis=0)\n",
    "    merged_outputs = np.zeros([total_len, total_output_dim], dtype='float32')\n",
    "    merged_outputs_mask = np.zeros([total_len, total_output_dim], dtype='float32')\n",
    "\n",
    "    startI = 0\n",
    "    startD = 0\n",
    "    start_ends = []\n",
    "    for output in outputs:\n",
    "        length, dim = output.shape\n",
    "        endD, endI = startD+dim, startI+length\n",
    "        \n",
    "        merged_outputs[startI:endI, startD:endD] = output\n",
    "        merged_outputs_mask[startI:endI, startD:endD] = 1.0\n",
    "        \n",
    "        start_ends.append(((startI, endI), (startD, endD)))\n",
    "        \n",
    "        startI = endI\n",
    "        startD = endD\n",
    "    display(start_ends)\n",
    "    return merged_inputs, merged_outputs, merged_outputs_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_multiple(indexes, dta_type):\n",
    "    dta_input = []\n",
    "    dta_output = []\n",
    "    for i in indexes:\n",
    "        input_tr, output_tr = load_data(dta_type, i)\n",
    "        input_tr_processed = downsample_input(input_tr)\n",
    "        \n",
    "        dta_input.append(input_tr_processed)\n",
    "        dta_output.append(output_tr)\n",
    "    \n",
    "    return merge_data_and_get_mask(dta_input, dta_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(input_tr, output_tr) = load_data('training', 2) \n",
    "(val_input, val_output) = load_data('validation', 2)\n",
    "\n",
    "display(input_tr.shape, output_tr.shape)\n",
    "display(val_input.shape, val_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(input_tr.shape)\n",
    "display(input_tr[0])\n",
    "display(input_tr.dtype)\n",
    "display(min(input_tr[0]), max(input_tr[0]), np.mean(input_tr[0]), np.std(input_tr[0]))\n",
    "display(min(input_tr[1]), max(input_tr[1]), np.mean(input_tr[1]), np.std(input_tr[1]))\n",
    "\n",
    "display(output_tr.shape)\n",
    "display(output_tr[0])\n",
    "display(output_tr.dtype)\n",
    "display(min(output_tr[0]), max(output_tr[0]), np.mean(output_tr[0]), np.std(output_tr[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def reshape_single_as_picture(input, size=31):\n",
    "    return np.reshape(input, (size, size))\n",
    "\n",
    "def as_single_picture(input, size=31):\n",
    "    return Image.fromarray(reshape_single_as_picture(input, size), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(as_single_picture(input_tr[0]))\n",
    "display(as_single_picture(input_tr[2]))\n",
    "display(as_single_picture(input_tr[700]))\n",
    "display(as_single_picture(input_tr[-1]))\n",
    "display(as_single_picture(input_tr[-2]))\n",
    "\n",
    "display(input_tr[0]*1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data trasnform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_input(dta):\n",
    "    def process_single(pic):\n",
    "        resized_pic = as_single_picture(pic).resize(size=(15, 15), resample=Image.BICUBIC)\n",
    "        return np.array(resized_pic).reshape(15*15)\n",
    "    return np.array([process_single(pic) for pic in dta])\n",
    "\n",
    "def normalize_input(dta):\n",
    "    return (dta/np.std(dta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr_processed = downsample_input(input_tr)\n",
    "display(as_single_picture(input_tr_processed[0], 15))\n",
    "display(as_single_picture(input_tr_processed[2], 15))\n",
    "display(as_single_picture(input_tr_processed[-1], 15))\n",
    "display(as_single_picture(input_tr_processed[-2], 15))\n",
    "\n",
    "display(input_tr_processed[0], min(input_tr_processed[0]), max(input_tr_processed[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NDN3.NDNutils as NDNutils\n",
    "import NDN3.NDN as NDN\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        input_dims=[1, input_shape], \n",
    "        layer_sizes=[hls, 2*hls, output_shape],\n",
    "        ei_layers=[0, hls // 2],\n",
    "        normalization=[0], \n",
    "        layer_types=['normal','normal','normal'], \n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(train_input, train_output, \n",
    "                  larg, opt_params, hsm_params, \n",
    "                  test_input = None, test_output = None, \n",
    "                  train_data_filters=None, test_data_filters=None):\n",
    "    time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    display(time_str)\n",
    "    \n",
    "    train_len, _ = train_input.shape\n",
    "    if test_input is not None:\n",
    "        input = np.concatenate([train_input, test_input], axis=0)\n",
    "        output = np.concatenate([train_output, test_output], axis=0)\n",
    "        data_filters = np.concatenate([train_data_filters, test_data_filters], axis=0) if train_data_filters is not None else None\n",
    "        test_len, _ = test_input.shape\n",
    "    else:\n",
    "        input = train_input\n",
    "        output = train_output\n",
    "        data_filters = train_data_filters\n",
    "        opt_params['early_stop'] = 0 # If we don't have test data -> shoudn't be early stopping (could early stop on train)\n",
    "        test_len = 0  \n",
    "        \n",
    "    train_indxs = np.array(range(train_len))\n",
    "    test_indxs = np.array(range(train_len, train_len + test_len)) if test_len > 0 else None\n",
    "    \n",
    "    hsm = NDN.NDN(hsm_params, noise_dist='poisson')\n",
    "    hsm.train(\n",
    "        input_data=input, \n",
    "        output_data=output, \n",
    "        train_indxs=train_indxs, \n",
    "        test_indxs=test_indxs, \n",
    "        data_filters=data_filters,\n",
    "        learning_alg=larg, \n",
    "        opt_params=opt_params, \n",
    "        output_dir=f\"logs/{time_str}\"\n",
    "    )\n",
    "    \n",
    "    return hsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(a, b, mask=None):\n",
    "    import scipy.stats\n",
    "    assert a.shape == b.shape\n",
    "    \n",
    "    mask = mask if mask is not None else np.ones(a.shape)\n",
    "    c = [\n",
    "        scipy.stats.pearsonr(\n",
    "            a[mask[:, i]==1,i], \n",
    "            b[mask[:, i]==1,i]\n",
    "        )[0] \n",
    "        for i \n",
    "        in range(a.shape[1])\n",
    "    ]\n",
    "    \n",
    "    return np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some_first_layers(hsm):\n",
    "    import matplotlib.pyplot as plt  # plotting\n",
    "\n",
    "    nrows, ncols = 5, 8\n",
    "\n",
    "    fig, _ = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    fig.set_size_inches(12, 6)\n",
    "    size_of_weights, neurons_or_filters = hsm.networks[0].layers[0].weights.shape\n",
    "    for neuron_i in range(min(nrows * ncols, neurons_or_filters)):\n",
    "        plt.subplot(nrows, ncols, neuron_i+1)\n",
    "        w = hsm.networks[0].layers[0].weights[:,neuron_i]     # get weight of a single neuron/filter in conv\n",
    "        w_size = int(np.sqrt(size_of_weights))                # assume 1:1 aspect ratio\n",
    "        plt.imshow(np.reshape(w, (w_size, w_size)), cmap='Greys', interpolation='none', vmin=-max(abs(w)), vmax=max(abs(w)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stat_matrix(string, stat):\n",
    "    display(string + \": \" + str(np.mean(stat)))\n",
    "    display(stat)\n",
    "    \n",
    "def evaluate_all(hsm, input, golden, data_filters=None):\n",
    "    dta_len, _ = input.shape\n",
    "    data_filters = data_filters if data_filters is not None else np.ones(golden.shape)\n",
    "\n",
    "    nuladj_eval = hsm.eval_models(input_data=input, output_data=golden, data_indxs=np.array(range(dta_len)),\n",
    "                           data_filters=data_filters, \n",
    "                           nulladjusted=False) # nulladjusted=True doens't work well with data_filters\n",
    "    print_stat_matrix(\"Eval \", nuladj_eval)\n",
    "    \n",
    "    result = hsm.generate_prediction(input)\n",
    "    result = np.multiply(result, data_filters)\n",
    "    \n",
    "    corr = get_correlation(result, golden, data_filters)\n",
    "    corr[np.isnan(corr)] = 0\n",
    "    print_stat_matrix(\"Corr\", corr)\n",
    "    \n",
    "    std_result, std_golden = np.std(result, axis=0), np.std(golden, axis=0)\n",
    "    print_stat_matrix(\"STD result\", std_result)\n",
    "    print_stat_matrix(\"STD golden\", std_golden)  \n",
    "    \n",
    "    return result, nuladj_eval, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stat_scalar(string, stat):\n",
    "    display(string + \": \" + str(stat))\n",
    "\n",
    "def evaluate_output_neuron(result, golden, corr):  \n",
    "    print_stat_scalar(\"Corr\", corr)\n",
    "\n",
    "    print_stat_scalar(\"STD result\", np.std(result, axis=0))\n",
    "    print_stat_scalar(\"STD golden\", np.std(golden, axis=0))\n",
    "    \n",
    "    print_stat_scalar(\"Mean result\", np.mean(result, axis=0))\n",
    "    print_stat_scalar(\"Mean golden\", np.mean(golden, axis=0))\n",
    "\n",
    "    display(\"Example result\", result[:15])\n",
    "    display(\"Example golden\", golden[:15])\n",
    "    \n",
    "def evaluate_best_corr_neuron(result, golden, data_filters=None):\n",
    "    data_filters = data_filters if data_filters is not None else np.ones(golden.shape)\n",
    "    \n",
    "    corr = get_correlation(result, golden, data_filters)\n",
    "    corr[np.isnan(corr)] = 0\n",
    "    \n",
    "    i = np.nanargmax(corr)\n",
    "    print_stat_scalar(\"Argmax i\", i)\n",
    "    \n",
    "    mask = data_filters[:,i] == 1 \n",
    "    evaluate_output_neuron(result[mask, i], golden[mask, i], corr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [2], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [2], 'validation')\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'lbfgs', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'maxiter': 2000, 'display': True, 'ftol': 2.220446049250313e-11},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 16, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 500, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 16, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 500, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 8, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 500, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr, hls=20)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 8, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr, hls=80)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 8, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 500, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        input_dims=[1, input_shape], \n",
    "        layer_sizes=[5, 2*hls, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','normal','normal'], \n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        input_dims=[1, input_shape], \n",
    "        layer_sizes=[5, 10, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'], \n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[3, 5, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'],\n",
    "        #shift_spacing=[2, 2, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    "\n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[3, 5, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'],\n",
    "        shift_spacing=[2, 2, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[5, 3, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'],\n",
    "        shift_spacing=[2, 2, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "hsm_params = get_hsm_params(input_tr_processed, output_tr, hls=40)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[10, 5, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'],\n",
    "        shift_spacing=[2, 2, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[10, hls, output_shape],\n",
    "        ei_layers=[0, hls//2],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','normal','normal'],\n",
    "        shift_spacing=[2, 2, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[0,1,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[10, hls*2, output_shape],\n",
    "        ei_layers=[0, hls//2],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','normal','normal'],\n",
    "        shift_spacing=[2, 0, 0],\n",
    "        conv_filter_widths=[3, 0, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[0,1,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hsm_params_custom(input, output, hls=40):\n",
    "    _, output_shape = output.shape\n",
    "    _, input_shape = input.shape\n",
    "    display(f\"in: {input_shape} out: {output_shape}\")\n",
    "\n",
    "    d2x = 0.0005\n",
    "    l1 = 0.000001\n",
    "\n",
    "    hsm_params = NDNutils.ffnetwork_params(\n",
    "        verbose=False,\n",
    "        input_dims=[1, 15, 15], \n",
    "        layer_sizes=[20, 10, output_shape],\n",
    "        ei_layers=[0, 0],\n",
    "        normalization=[0], \n",
    "        layer_types=['conv','conv','normal'],\n",
    "        shift_spacing=[2, 3, 0],\n",
    "        conv_filter_widths=[3, 3, 0],\n",
    "        reg_list={\n",
    "            'd2x':[d2x,None,None],\n",
    "            'l1':[l1,None,None],\n",
    "            'max':[None,None,100]})\n",
    "    hsm_params['weights_initializers']=['normal','normal','normal']\n",
    "    hsm_params['normalize_weights']=[1,0,0]\n",
    "    \n",
    "    return hsm_params\n",
    "\n",
    "input_tr_processed, output_tr, output_tr_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'training')\n",
    "input_val_processed, output_val, output_val_mask = load_data_multiple(\n",
    "    [1, 2, 3], 'validation')\n",
    "\n",
    "\n",
    "hsm_params = get_hsm_params_custom(input_tr_processed, output_tr, hls=80)\n",
    "display(hsm_params)\n",
    "hsm = train_network(\n",
    "    input_tr_processed, output_tr,\n",
    "    'adam', \n",
    "    {'batch_size': 2, 'use_gpu': False, 'epochs_summary': 10, 'epochs_training': 300, 'learning_rate': 0.5e-4},\n",
    "    hsm_params,\n",
    "    input_val_processed, output_val,\n",
    "    output_tr_mask, output_val_mask\n",
    ")\n",
    " \n",
    "display(hsm)\n",
    "\n",
    "res, naeval, corr = evaluate_all(hsm, input_val_processed, output_val, output_val_mask)\n",
    "show_some_first_layers(hsm)\n",
    "evaluate_best_corr_neuron(res, output_val, output_val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
